A/B experiments
    action advantage
    action step
    evaluating
    exploiting versus exploring
    Actor-Critic algorithms
    Advantage Actor-Critic
    A2C
    Asynchronous Advantage Actor-Critic
    A3C
    Dueling DQN algorithm
    genetic algorithms
    off-policy algorithms
    on-policy algorithms
    Proximal Policy Optimization
    PPO
    REINFORCE algorithms
    Soft Actor-Critic algorithm
    Value Iteration algorithm
    Approximate Q-Learning
    Atari preprocessing
    batched action step
    batched time step
    batched trajectory
    Bellman Optimality Equation
    boundary transitions
    catastrophic forgetting
    collect policy
    mean squared error
    credit assignment problem
    curiosity-based exploration
    training loops
    datasets
    Deep Q-Learning
    Double DQN
    Dueling DQN
    fixed Q-Value targets
    prioritized experience replay
    deep Q-networks
    DQNs
    deques
    discount factors
    Double Dueling DQN
    DQN agents
    Dynamic Programming
    exploration policy
    importance sampling
    IS
    Markov chains
    Markov Decision Processes
    MDP
    installing
    observers
    online model
    OpenAI Gym
    optimal state value
    policies
    policy gradients
    PG
    policy parameters
    policy search
    policy space
    PER
    Q-Learning
    Approximate Q-Learning and Deep Q- Learning
    implementing
    Q-Value Iteration
    Q-Values
    queries per second
    QPS
    Rainbow agent
    evaluating actions
    neural network policies
    optimizing rewards
    Temporal Difference Learning
    TF-Agents library
    replay buffers
    replay memory
    sample inefficiency
    simulated environments
    state-action values
    stochastic policy
    target model
    TD error
    TD target
    TD Learning
    TensorFlow model deployment at scale
    terminal state
    collect driver
    environment specifications
    environment wrappers
    environments
    replay buffer and observer
    training architecture
    training metrics
    trajectories
    trajectory
    undiscounted rewards