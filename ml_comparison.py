# -*- coding: utf-8 -*-
"""ML_comparison.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IyPMPtdoMxVl4surVKm_v12O_vpxPntL
"""

# -*- coding: utf-8 -*-
"""ML_comparison.ipynb
"""

import spacy
import pytextrank
import re
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from sentence_transformers import SentenceTransformer, util
from rake_nltk import Rake
import nltk
import yake
from transformers import AutoModel, AutoTokenizer
from keybert import KeyBERT
import itertools
from matplotlib.patches import FancyArrowPatch
import networkx as nx
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer

# Download required NLTK resources
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

# Load SentenceTransformer with a fine-tuned KeyBERT model
custom_model_path = r"C:\Users\Tyler\PycharmProjects\ML7267GroupProject\fine_tuned_keybert_inspec"
embedder = SentenceTransformer(custom_model_path)
kw_model = KeyBERT(model=embedder)

# Text cleaning function
def clean_text(text):
    text = re.sub(r'```.*?```', '', text, flags=re.DOTALL)  # Remove code blocks
    text = re.sub(r'\n+', ' ', text)  # Flatten newlines
    text = re.sub(r'[^\x00-\x7F]+', ' ', text)  # Remove non-ASCII
    text = re.sub(r'https?://\S+', '', text)  # Remove URLs
    return text.strip()

def clean_rake_keywords(keywords):
    """ Post-process RAKE keywords: remove garbage phrases """
    cleaned = []
    for kw in keywords:
        kw = kw.strip()
        if re.search(r'\d', kw):
            continue
        if '_' in kw:
            continue
        if len(kw.split()) < 2:
            continue
        if len(kw) < 5:
            continue
        cleaned.append(kw)
    return cleaned

# Lemmatization function
def lemmatize_phrase(phrase):
    """ Lemmatize a full phrase """
    lemmatizer = WordNetLemmatizer()
    return ' '.join([lemmatizer.lemmatize(word) for word in phrase.split()])

# Load text data from file
filepath = r"C:\Users\Tyler\PycharmProjects\ML7267GroupProject\fine_tuned_keybert_inspec\ch1.txt"
top_n = 15
with open(filepath, 'r', encoding='utf-8') as f:
    text = f.read()
cleaned_text = clean_text(text)

# Extract ground truth keywords (chapter-wise)
chapters = {}
with open(r"C:\Users\Tyler\PycharmProjects\ML7267GroupProject\fine_tuned_keybert_inspec\index_keywords.txt", 'r', encoding='utf-8') as file:
    current_chapter = None
    for line in file:
        line = line.strip()
        if line.startswith('Chapter'):
            current_chapter = line
            chapters[current_chapter] = []
        elif line:
            chapters[current_chapter].append(line)

ground_truth = chapters['Chapter 1']
print(ground_truth)

# Filter out similar keywords based on cosine similarity
def filter_similar_keywords(keywords, threshold=0.85):
    embeddings = embedder.encode(keywords)
    similarity_matrix = cosine_similarity(embeddings)

    final_keywords = []

    for i, keyword in enumerate(keywords):
        is_similar = False
        for j in range(i):
            if similarity_matrix[i][j] > threshold:
                is_similar = True
                break
        if not is_similar:
            final_keywords.append(keyword)

    return final_keywords

# Textrank with SpaCy
nlp = spacy.load("en_core_web_trf")
nlp.add_pipe("textrank")

doc = nlp(cleaned_text)
keyphrases = [(phrase.text, phrase.rank) for phrase in doc._.phrases]
sorted_keyphrases = sorted(keyphrases, key=lambda x: x[1], reverse=True)
keywords_textrank = filter_similar_keywords([keyword for keyword, _ in sorted_keyphrases])[:top_n]

# RAKE
stopwords = set(nltk.corpus.stopwords.words('english'))
rake = Rake(stopwords=stopwords, max_length=3, min_length=1)
rake.extract_keywords_from_text(cleaned_text)
keywords_rake = rake.get_ranked_phrases_with_scores()
keywords_rake = filter_similar_keywords([keyword for score, keyword in keywords_rake])
keywords_rake = clean_rake_keywords(keywords_rake)[:top_n]

# YAKE
language = "en"
max_ngram_size = 3
deduplication_threshold = 0.3
yake_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold)
keywords_yake = yake_extractor.extract_keywords(cleaned_text)
keywords_yake = sorted(keywords_yake, key=lambda x: x[1], reverse=True)
keywords_yake = filter_similar_keywords([keyword for keyword, score in keywords_yake])[:top_n]

# KeyBERT with custom model
kw_bert = kw_model.extract_keywords(cleaned_text, keyphrase_ngram_range=(1, 3), stop_words='english', top_n=30)
sorted_bert = sorted(kw_bert, key=lambda x: x[1], reverse=True)
keywords_bert = filter_similar_keywords([keyword for keyword, score in sorted_bert])[:top_n]

# Evaluation of Keywords
def evaluate_keywords(predicted, ground_truth, threshold=0.7):
    pred_lem = [lemmatize_phrase(p.lower()) for p in predicted]
    gt_lem = [lemmatize_phrase(g.lower()) for g in ground_truth]

    matches = 0
    unmatched_gt = set(gt_lem)

    for pred_kw in pred_lem:
        for gt_kw in list(unmatched_gt):
            vec = TfidfVectorizer().fit([pred_kw, gt_kw])
            vecs = vec.transform([pred_kw, gt_kw])
            sim = cosine_similarity(vecs[0], vecs[1])[0][0]
            if sim >= threshold:
                matches += 1
                unmatched_gt.remove(gt_kw)
                break

    precision = matches / 15 if predicted else 0
    recall = matches / len(ground_truth) if ground_truth else 0
    f1 = (2 * precision * recall) / (precision + recall + 1e-10) if (precision + recall) > 0 else 0

    return precision, recall, f1

# Evaluation Results
methods = {
    "Textrank": keywords_textrank,
    "RAKE": keywords_rake,
    "YAKE": keywords_yake,
    "KeyBERT (MiniLM)": keywords_bert
}

results = {}

print("\n--- Evaluation Results ---\n")
for method_name, keywords in methods.items():
    precision, recall, f1 = evaluate_keywords(keywords, ground_truth)
    results[method_name] = (precision, recall, f1)
    print(f"{method_name} -> Precision: {precision:.2f} | Recall: {recall:.2f} | F1 Score: {f1:.2f}")


def get_cooccurring_pairs(keywords, text):
    cooccur_pairs = set()
    paragraphs = text.split('\n')

    for para in paragraphs:
        para_text = para.lower()
        present = [kw for kw in keywords if kw.lower() in para_text]
        cooccur_pairs.update(itertools.combinations(present, 2))

    return list(cooccur_pairs)

def filter_by_semantic_similarity(pairs, threshold=0.5):
    filtered = []
    for a, b in pairs:
        emb1 = embedder.encode(a, convert_to_tensor=True)
        emb2 = embedder.encode(b, convert_to_tensor=True)
        sim = util.pytorch_cos_sim(emb1, emb2).item()
        if sim > threshold:
            filtered.append((a, b, sim))
    return filtered

def assign_relationships(filtered_pairs,text):
    relationships = []
    doc = nlp(text)
    for a, b, _ in filtered_pairs:
        found_relation = "related-to"

        for sent in doc.sents:
            sent_text = sent.text.lower()
            if a.lower() in sent_text and b.lower() in sent_text:
                if "is a" in sent_text or "type of" in sent_text:
                    found_relation = "is-a"
                    break
                elif "required for" in sent_text or "prerequisite" in sent_text:
                    found_relation = "prerequisite-of"
                    break
        relationships.append((a, b, found_relation))
    return relationships

def build_concept_map(relations, all_keywords):
    G = nx.DiGraph()
    for kw in all_keywords:
        G.add_node(kw)
    for src, tgt, label in relations:
        G.add_edge(src, tgt, label=label)
    threshold = 0.45
    embeddings = embedder.encode(all_keywords, convert_to_tensor=True)
    for i in range(len(all_keywords)):
        for j in range(len(all_keywords)):
            if i != j and not G.has_edge(all_keywords[i], all_keywords[j]) and not G.has_edge(all_keywords[j], all_keywords[i]):
                sim_score = util.pytorch_cos_sim(embeddings[i], embeddings[j]).item()
                if sim_score > threshold:
                    G.add_edge(all_keywords[i], all_keywords[j], weight=sim_score)
    return G

def avoid_node_overlap(pos, node_size=1000, min_distance=0.2, iterations=50):
    nodes = list(pos.keys())
    for _ in range(iterations):
        moved = False
        for i in range(len(nodes)):
            for j in range(i + 1, len(nodes)):
                u, v = nodes[i], nodes[j]
                dx = pos[v][0] - pos[u][0]
                dy = pos[v][1] - pos[u][1]
                dist = np.sqrt(dx**2 + dy**2)
                if dist < min_distance:
                    angle = np.arctan2(dy, dx)
                    move_dist = (min_distance - dist) / 2
                    pos[u][0] -= np.cos(angle) * move_dist
                    pos[u][1] -= np.sin(angle) * move_dist
                    pos[v][0] += np.cos(angle) * move_dist
                    pos[v][1] += np.sin(angle) * move_dist
                    moved = True
        if not moved:
            break
    return pos

def draw_concept_map(G):
    plt.figure(figsize=(12, 10))

    pos = nx.spring_layout(G, k=0.8, iterations=100)
    pos = {node: list(coord) for node, coord in pos.items()}  # make mutable
    pos = avoid_node_overlap(pos, min_distance=0.25)

    nx.draw_networkx_nodes(G, pos, node_size=1000, node_color='lightblue')
    nx.draw_networkx_labels(G, pos, font_size=10)

    ax = plt.gca()
    for u, v, data in G.edges(data=True):
        if u == v:
            continue

        src, dst = pos[u], pos[v]
        dx, dy = dst[0] - src[0], dst[1] - src[1]
        norm = np.sqrt(dx**2 + dy**2)
        shrink = 0.05

        start = (src[0] + dx / norm * shrink, src[1] + dy / norm * shrink)
        end = (dst[0] - dx / norm * shrink, dst[1] - dy / norm * shrink)

        arrow = FancyArrowPatch(start, end,
                                arrowstyle='->',
                                color='gray',
                                linewidth=1,
                                mutation_scale=15,
                                connectionstyle='arc3,rad=0.05')
        ax.add_patch(arrow)

    edge_labels = nx.get_edge_attributes(G, 'label')
    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='darkred', font_size=9)

    plt.title("Educational Concept Map", fontsize=14)
    plt.axis('off')
    plt.tight_layout()
    plt.show()

cooccur_pairs_textrank = get_cooccurring_pairs(keywords_textrank, text)
cooccur_pairs_rake = get_cooccurring_pairs(keywords_rake, text)
cooccur_pairs_yake = get_cooccurring_pairs(keywords_yake,text)
cooccur_pairs_bert = get_cooccurring_pairs(keywords_bert, text)

semantically_related_textrank = filter_by_semantic_similarity(cooccur_pairs_textrank)
semantically_related_rake = filter_by_semantic_similarity(cooccur_pairs_rake)
semantically_related_yake = filter_by_semantic_similarity(cooccur_pairs_yake)
semantically_related_bert = filter_by_semantic_similarity(cooccur_pairs_bert)

relations_textrank = assign_relationships(semantically_related_textrank,text)
relations_rake = assign_relationships(semantically_related_rake,text)
relations_yake = assign_relationships(semantically_related_yake,text)
relations_bert = assign_relationships(semantically_related_bert,text)

G_textrank = build_concept_map(relations_textrank, keywords_textrank)
G_rake = build_concept_map(relations_rake, keywords_rake)
G_yake = build_concept_map(relations_yake, keywords_yake)
G_bert = build_concept_map(relations_bert,keywords_bert)

draw_concept_map(G_textrank)

draw_concept_map(G_rake)

draw_concept_map(G_yake)

draw_concept_map(G_bert)

